# 深度学习 - 神经网络的训练过程

神经网络的学习是一个从数据中学习的过程，神经网络应当根据数据自发调整自己的参数。这里面就存在一个特性，即神经网络可以自动提取特征，而且这个过程是对于所有的问题都通用的，进行一个数据直接通向结果的学习。

## 训练数据和测试数据

在训练神经网络的时候，我们需要准备两种数据，一种是训练数据，另一种是测试数据。训练数据用于训练神经网络，测试数据用于测试神经网络的泛化能力。

训练数据又称为监督数据，用于训练神经网络。如果只有训练数据，神经网络就好出现过拟合，而泛化能力的衡量就需要测试数据。

## 损失函数

损失函数是用于衡量神经网络输出结果与真实结果之间的差距。一般情况下，会用到均方误差函数（Mean Squared Error）或者交叉熵误差函数（Cross Entropy Error）。

### 均方误差函数

对于一个手写识别问题，我们可能得到的相关如下：

```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

这里面的`y`表示的是神经网络的输出（通过softmax函数求出的可以视为概率的东西）。而`t`表示的是真实的结果。这里面把第`2`个元素设为`1`，表示的是真实的结果是`2`。这种表示方法叫做**one-hot表示**。

这个时候，我们可以使用均方误差函数来衡量这两个向量之间的差距：

$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$

上述的计算结果是`0.0975`。作为对比，如果`y`是`[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]`，那么计算结果是`0.5975`，远远大于`0.0975`。

### 交叉熵误差函数

交叉熵误差函数的定义如下：

$$
E = -\sum_{k} t_k \log y_k
$$

这个实际上只考虑了正确解标签的输出，并且正确解的概率越大，交叉熵误差越小。对于上面的例子，在预测结果是`2`的时候，交叉熵误差是`0.5108`，而在预测结果是`7`的时候，交叉熵误差是`2.3026`。

### mini-batch

学习过程中试用的是训练数据，针对训练数据计算损失函数，并且找到使得其值最小的参数。

> to be continued