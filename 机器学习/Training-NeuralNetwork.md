# 深度学习 - 神经网络的训练过程

神经网络的学习是一个从数据中学习的过程，神经网络应当根据数据自发调整自己的参数。这里面就存在一个特性，即神经网络可以自动提取特征，而且这个过程是对于所有的问题都通用的，进行一个数据直接通向结果的学习。

## 训练数据和测试数据

在训练神经网络的时候，我们需要准备两种数据，一种是训练数据，另一种是测试数据。训练数据用于训练神经网络，测试数据用于测试神经网络的泛化能力。

训练数据又称为监督数据，用于训练神经网络。如果只有训练数据，神经网络就好出现过拟合，而泛化能力的衡量就需要测试数据。

## 损失函数

损失函数是用于衡量神经网络输出结果与真实结果之间的差距。一般情况下，会用到均方误差函数（Mean Squared Error）或者交叉熵误差函数（Cross Entropy Error）。

引入损失函数而不是直接根据模型的准确度去跟进模型的训练情况，是因为损失函数是连续的，而模型的准确度是离散的，因此在些许的变化下，损失函数会有变化而最终的分类结果是不会改变的，因此损失函数是才能用来批判这次训练效果是好还是不好。

### 均方误差函数

对于一个手写识别问题，我们可能得到的相关如下：

```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

这里面的`y`表示的是神经网络的输出（通过softmax函数求出的可以视为概率的东西）。而`t`表示的是真实的结果。这里面把第`2`个元素设为`1`，表示的是真实的结果是`2`。这种表示方法叫做**one-hot表示**。

这个时候，我们可以使用均方误差函数来衡量这两个向量之间的差距：

$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$

上述的计算结果是`0.0975`。作为对比，如果`y`是`[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]`，那么计算结果是`0.5975`，远远大于`0.0975`。

### 交叉熵误差函数

交叉熵误差函数的定义如下：

$$
E = -\sum_{k} t_k \log y_k
$$

这个实际上只考虑了正确解标签的输出，并且正确解的概率越大，交叉熵误差越小。对于上面的例子，在预测结果是`2`的时候，交叉熵误差是`0.5108`，而在预测结果是`7`的时候，交叉熵误差是`2.3026`。

在实际的使用中，会在计算交叉熵的过程中，给`y`加上一个很小的值（如`1e-7`），以避免出现`log(0)`的情况，产生`inf`的结果。

```python
import numpy as np
import matplotlib.pyplot as plt

def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
print(cross_entropy_error(y, t))  # 0.510825457099338
```

实际运行与上述的结果是基本一致的。

### mini-batch

简单来说就是，如果将所有的数据集全部一起拿去训练，以交叉熵误差为例：

$$
E = -\frac{1}{N} \sum_{n=1}^{N} \sum_{k} t_k^{(n)} \log y_k^{(n)}
$$

那么计算量会非常大，因此我们可以将数据集分成若干个小的数据集，然后分批次进行训练。这种方法叫做mini-batch。

## 梯度下降法

### 梯度

简单来说就是，一个多元函数，对于每个变量的偏导数，构成一个向量，这个向量叫做梯度。梯度的方向是函数上升最快的方向，梯度的反方向是函数下降最快的方向。

在计算梯度的时候，通常采用数值微分去计算梯度，即对于给定的函数`f(x)`，我们可以通过以下的公式去计算：

$$
\frac{\partial f}{\partial x} \approx \frac{f(x + h) - f(x - h)}{2h}
$$

这里的`h`是一个很小的数值，通常取`1e-4`。这个数值理论上应该尽可能小，但是由于计算机计算浮点数的限制，如果`h`过小，可能会导致计算结果不准确，因此一般取`1e-4`。

```python
import numpy as np

def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # 创建与x同样形状的数组
    for idx in range(x.size):
        tmp_val = x[idx]  # 保存当前值
        x[idx] = tmp_val + h  # 增加h
        fxh1 = f(x)  # f(x+h)
        x[idx] = tmp_val - h  # 减少h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2 * h)  # 求导数
        x[idx] = tmp_val  # 恢复值
    return grad

def function_2(x):
    return x[0]**2 + x[1]**2

x = np.array([3.0, 4.0])
print(numerical_gradient(function_2, x))  # [6. 8.]
```

### 梯度下降法

梯度下降算法就是说，计算出当前的梯度，然后根据梯度的方向去更新参数，然后继续计算梯度，更新参数，直到收敛为止。每次更新参数的时候，实际的计算是：

$$
x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}
$$

$$
x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}
$$

这里的`η`是学习率，表示每次更新参数的步长。学习率过大，可能会导致参数更新过快，可能越过了极值点而难以收敛；学习率过小，可能会导致参数更新过慢，导致收敛速度慢。

## 学习算法的实现

在实现学习算法的时候，我们需要实现以下几个步骤：

1. mini-batch的划分
2. 计算损失函数
3. 计算梯度
4. 更新参数

由于选取mini-batch的方式经常是随机的，所以这种算法也叫做随机梯度下降法（Stochastic Gradient Descent，SGD）。

## 误差反向传播

### 计算图

计算图展示了一个复杂的计算过程，每一个节点表示一个操作，每一条边表示一个变量。对于每一个节点而言，它只需要知道它的输入和输出，并正确完成定义的计算，而不需要关注上游或者下游发生了什么。此外，它还有一个很大的优势，即很便于进行反向传播计算导数。

反向传播是，假设计算图的节点进行了一个$y=f(x)$的计算，那么反向传播就是在由结果倒推$\frac{\partial y}{\partial x}$的过程，并且把这个数值乘以传到下一个节点的值。这个过程是从最后一个节点开始，逐层向前推导的。

这个计算在数学上是成立的，因为求导的过程中，存在链式法则：

$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x}
$$
