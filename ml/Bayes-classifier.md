# 机器学习系列笔记 - 贝叶斯分类器

这是分类问题的第一个常见的方法，作为整个笔记的第一个内容。这是一个数理统计有关的方法。

## 贝叶斯定理

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

其中：

- $P(A|B)$是在给定B的情况下A的概率（后验概率，Posterior Probability）
- $P(B|A)$是在给定A的情况下B的概率（条件概率，Conditional Probability；或者称为似然度，Likelihood）
- $P(A)$是A的概率（先验概率，Prior Probability）
- $P(B)$是B的概率（证据，Evidence）

## 一个简单的例子

假设有一个分类问题，一共分了$n$类，分别为$c_1, c_2, \dots, c_n$。现在有一个样本，在各个特征上的表现为$x$。为了简单起见，假设我们只考虑一个特征，而且其可能的值还是可枚举的离散量。需要计算$x$属于$c_i$的概率，即$P(c_i|x)$。我们的数据集大概是这样的一个格式：

| 特征值 | 类别 |
| ------ | ---- |
| $x_1$  | $c_1$|
| $x_2$  | $c_1$|
| $x_3$  | $c_2$|
| $x_3$  | $c_1$|
| $\dots$| $\dots$|
| $x_m$  | $c_1$|

$$
P(c_i|x) = \frac{P(x|c_i)P(c_i)}{\sum_{j=1}^n P(x|c_j)P(c_j)}
$$

分母实际上就是$P(x)$，通过全概率公式可以如此计算。

那么在这个具体的问题下面，首先假定数据集已经足够大，可以根据**大数定律**，以频率估计概率。计算每一个$P(x_i|c_j)$的值，以及$P(c_j)$的值。比如说，在整个数据集里面$c_1$类一共占比$0.3$，那么$P(c_1)=0.3$；而假设在$c_1$类里面，特征$x_1$出现的频率为$0.8$，$x_3$出现的概率为$0.05$，那么$P(x_1|c_1)=0.8$，$P(x_3|c_1)=0.05$；假设全部样本中特征$x_1$的频率是$0.25$，则概率也就视为$0.25$

但是以上是站在大数定律的基础上思考，但是对于每个子类的每个取值，可能数量都会少一些，那就不好直接用频率来估计概率了。这个问题下文再探讨。

对于一个给定的样本$x$，假设他就是$x_1$，那么$P(c_1|x) = \frac{P(x_1|c_1)P(c_1)}{P(x_1|c_1)P(c_1) + P(x_1|c_2)P(c_2) + \dots + P(x_1|c_n)P(c_n)} = \frac{0.8 \times 0.3}{0.25} = 0.96$

继续注意到，在已知$x_1$的情况下，每个类别的分母都是$P(x_1)$，所以可以直接只比较分子的大小，从而比较每个后验概率的大小关系。最终，比对出后验概率最大的，就是这个样本被认为属于的类别。

在这个例子里面，所有的概率可以直接通过统计学得到，因为只有一个特征，而且其取值是离散的，可以直接计算频率。那么在更加一般的场景下面，频率的计算就会需要其他的方法了。

## 更一般的概率计算

假设存在一个特征，他的取值是连续的，那么直接通过数频数来计算频率进而估计概率的思路就不再成立，因为频率都是$0$。在这个情况下，就需要通过假定一个分布，然后估计参数。

比如说，假设这个单一特征$x$是连续型，且可以认为服从正态分布（通常来说，自然状态下的分布很多都是近似服从正态分布）。正态分布的概率密度函数为：

$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

因此只需要根据每一个类别，求得他所对应的$\mu$（均值）和$\sigma$（标准差），对于接下来如何计算似然度，就需要利用**最大似然估计**的方法了，下文再说。

此外可能还存在一种情况，特征的表现形式并不是数字，而是文字等。可以考虑，在可枚举的情况下，直接计算频率。还有其他将文本转化为数的方式，可以在自然语言处理的有关内容中找到。恕不赘述。

## 风险和加权

一个非常简单的例子，假设有一个根据体检指标判断有没有患病的分类任务，已知患病的概率为$0.01$。有一个非常简单而且高效的判断方案，就是所有的人都视为没有患病。这样的分类器的准确率是$0.99$。

这个数字看起来很高，但是实际上该模型毫无价值。因为假阴性的代价会远远大于假阳性的代价。因此，需要引入风险的概念，来综合考虑这两种错误的代价。比方说，假阳性的代价是假阴性代价的百分之一。我们对于上述的模型进行一个改善：

$$
R(c_i|x) = \sum_{j=1}^n \lambda_{i,j}P(c_j|x)
$$

其中$\lambda_{i,j}$是将一个真实分类为$c_j$的样本误分类为$c_i$的代价。这样，我们就可以通过计算风险来选择一个最优的分类器。

在这种情况下，所需要解决的问题就是，寻找一个判断准则，使得对于每个样本$x$，其被分类为$h(x) = c_i$，那么定义风险为：

$$
R(h) = \mathbb{E}_x[R(h(x)|x)]
$$

其中$R(h(x)|x)$是分类器$h$下的风险，它的期望（很显然是针对变量$x$的期望，式子里面的$h(x)$就是被分类的结论，某一个$c_i$）就定义为这个分类器的风险。现在的任务就是寻找这个期望最小的分类器。很显然对于每一个样本，其风险最小，那么这个整体的风险就最小，此时的这个分类器称为**贝叶斯最优分类器**，与之对应的总体风险就是**贝叶斯风险**，用1减去这个风险就是理论可以达到的最佳的精度。

那么就在最特殊的情况，即不考虑不同错误之间的风险存在差距的情况下：

$$
\lambda_{i,j} = \begin{cases}
0 & i=j\\
1 & i\neq j
\end{cases}
$$

那么

$$
R(c_i|x) = 1 - P(c_i|x)
$$

（推导很显然，就是相加了1倍的除了正确分类以外的概率，那就是1减去正确分类的概率）

于是问题又回到了一开始的问题，计算后验概率并比对，选择最小值。

## 极大似然估计

这个时候可能存在这样的问题，在每一个子分类下，并没有足够多的数目，因此大数定律不适用，误差会很大。

为了估计条件概率，存在以下的假设：

- $P(x|c_i)$具有某种特定的形式，并且由一个参数向量$\theta_i$控制
- $\theta_i$是未知的，但是客观存在一个固定值

令$D_c$表示属于类别$c$的样本集合，并且假设样本之间**独立**分布，那么可以通过极大似然估计来估计$\theta_c$。即：

$$
P(D_c|\theta_c) = \prod_{x\in D_c}P(x|\theta_c)
$$

就是把每个样本分到这个类里面的概率相乘，得到的是这个类别有这些样本的概率。这个$\theta_c$参数可以视为这样子分布的条件。

那么对于这个$\theta_c$，我们要求找到其取值使得上述的式子取极大值。为了方便求解，因为很多的概率密度函数都是指数形式，包括高斯分布的$f(x) = e^{-\frac{(x-\mu)^2}{2\sigma^2}}$，因此可以对上述的式子取对数，然后求导，令导数为0，解出$\theta_c$：

$$
\begin{aligned}
LL(\theta_c) &= \log P(D_c|\theta_c) = \sum_{x\in D_c}\log P(x|\theta_c)\\
\^{\theta_c} &= \arg\max_{\theta_c} LL(\theta_c)\\
\frac{\partial LL(\^{\theta_c})}{\partial \theta_c} &= 0
\end{aligned}
$$

求解这个，就可以得到$\theta_c$的估计值。在一个特别常见的情况下，假设是服从$(\mu, \sigma^2)$的高斯分布，那么可以得到：

$$
\begin{aligned}
\mu &= \frac{1}{|D_c|}\sum_{x\in D_c}x = \bar{x}\\
\sigma^2 &= \frac{1}{|D_c|}\sum_{x\in D_c}(x-\mu)^2
\end{aligned}
$$

也就是说，通过极大似然法得到的正态分布均值就是样本均值，方差就是样本方差。这很符合直觉。因此，即便是数目很少的样本，也可以直接将这些离散的点通过极大似然估计，得到一个连续的分布。

当然了这个参数化方法估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。这部分内容下文再说。

## 朴素贝叶斯分类器

还是这个假设，选取的特征是独立的，那么可以得到：

$$
P(x|c_i) = \frac{p(c_i)}{p(x)} \prod_{j=1}^m P(x_j|c_i)
$$

前面那个是参数不管他，后面那个就是各个特征的概率的乘积。在这个里面，需要找到使得这个概率最大的$c_i$，也就是：

$$
\^c = \arg\max_{c_i} P(c_i) \prod_{j=1}^m P(x_j|c_i)
$$

先验概率$P(c_i)$可以通过频率估计，而后验概率可以通过上文所述的方式计算：

- 对于离散型特征，可以直接计算频率；
- 对于连续型特征，可以通过极大似然估计得到。通过假设特征服从某种分布（通常是高斯分布），计算参数，并且计算在$x_i$处的概率密度函数。

至于为什么连续的时候是概率密度函数，可以简单这么考虑，因为连续值取任何一个具体的数的概率都是0，因此在不同者做比较的时候，就可以相除，发生0/0的情况，可以分别对其求导，因此比值就是概率密度函数。

## 一些分析

首先假设可以完全准确地得到所有的样本作为训练集，而这样得到的贝叶斯分类器是所有的机器学习模型里面，能力的上限。

这么说是因为，对于所有的可以得到的信息，在最极端的情况下把所有的特征都视为离散的，那么这就是一个“花名册”，来一个样本进去查找到那一行就可以了；如果仍然无法区分，那么就无法根据已知的信息得到更准确的分类。举个例子，假设得到了全世界男性和女性的所有的身高的信息，并且构成了一个贝叶斯分类器，但是根据一个已知的身高只能猜测他是男性还是女性，但是实际上不可能确认，因为身高相同的个体太多了，不存在更多的信息了。这是数据集的问题。

那么既然这是最优的分类器，那么为什么还有其他的分类器呢？这是因为在实际的情况下，数据集是有限的，而且所得到的数据集的每个类别的概率根本不是整个世界上的概率。举个例子，医院里面对于诊断疾病的数据，大量的没有疾病的人不会去医院贡献自己的数据，因此医院里面数据得到的患病率会特别明显地大于真实的患病率。

因此，贝叶斯分类器对于数据可靠性要求特别高，他要求数据的特征完全互相独立，并且数据的分布是和真实情况同分布的子集。这两个要求会特别高，因此在实际的情况下，贝叶斯分类器的效果远远不是最优的。
